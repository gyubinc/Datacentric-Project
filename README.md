# 1. 프로젝트  개요

### A. 프로젝트 주제

<aside>
💡 **주제 분류 프로젝트**(Topic Classification task) / Data-Centric

</aside>

- 뉴스의 헤드라인을 통해 그 뉴스가 어떤 Topic을 갖는지를 분류해내는 Task 입니다.
- Data-Centric 의 취지에 맞게, 베이스라인 모델의 수정 없이 오로지 데이터의 수정으로만 성능 향상을 이끌어내야 합니다

### B. 데이터셋

- **데이터 유형 (Source)** : KLUE-TC(YNAT)
- **Train 데이터 개수 :** 45,678개
- **Test 데이터 개수** : 9,107개
- **데이터 특징** : G2P를 이용한 text perturbation으로 생성된 Noise Data와 Labeling Error가 있다.
- **Label(Target) 유형**

| 0 | IT과학 | 1 | 경제 |
| --- | --- | --- | --- |
| 2 | 사회 | 3 | 생활문화 |
| 4 | 세계 | 5 | 스포츠 |
| 6 | 정치 |  |  |

# 2. 프로젝트  팀  구성  및  역할

- 김효연 : 크롤링, G2P증강, RD증강
- 서유현 : Back Translation, chatAug 이용한 데이터 증강
- 손무현 : 크롤링데이터 활용하여 비교 실험, 증강 실험
- 이승진 : chatGPT 증강, CleanLab을 통한 데이터 클리닝/ 필터링
- 최규빈 : 언더샘플링 기반 레이블 밸런싱
- 황지원 : 코드관리, 데이터 라벨링 및 샘플링 (Hanspell, BT), 데이터 클리닝

# 3. 프로젝트  수행  절차  및  방법

### A. 팀 목표 설정

**(1주차)** 대회 간 팀 규칙을 정하였고 협업을 위한 버전관리 방법론으로 Github Flow를 선정하였습니다. 또한 베이스코드를 수정하여 다양한 케이스의 데이터를 학습하기 편하도록 작업을 진행하였습니다.

데이터 분석과 함께 Back-translation, 맞춤법 검사 등 데이터 전처리와 EDA 기법을 활용한 데이터 증강 작업을 진행하였습니다. 또한 크롤링과 라벨링을 통해 에러 데이터를 검수하여 데이터를 수정하였고 이를 학습 데이터에 반영하였습니다.

---

**(2주차)** 이전에 데이터 전처리를 거쳐 만든 증강 데이터에 대해 학습을 진행하였습니다. 모델/토크나이저에 대해 수정할 수 없는 만큼 학습 데이터를 증강하고자 작업을 진행하였습니다. 

Negative Sampling와 EDA, Oversampling/Undersampling 각각을 진행하였고, SOTA 결과를 얻기위해 다양한 실험을 진행하였습니다.

---

# 4. 프로젝트  수행  결과

## 1. 데이터 분석

- **데이터 source와 특징**
    - 이번 대회에 사용한 데이터는 KLUE의 ynat(Topic Classification, 주제 분류) Dataset을 사용하였습니다.
    - 분류한 Labeling으로는 `생활문화(Society)`, `스포츠(Sports)`, `세계(World)`, `정치(Politics)`, `경제(Economy)`, `IT과학(IT/Science)`, `사회(Society)` 7가지의 주제로 분류하였습니다.
    - 데이터에서는 각 라벨을 숫자로 변환해 사용하였으며, 다음과 같은 라벨과 분포를 보여줍니다.
- **Raw Data 확인**
    - 일부 데이터에서 맞춤법에 맞지 않게 g2p 방식을 사용하여 데이터를 변형한 것을 확인하였습니다.
    - 이러한 에러 데이터가 얼마나 있는지 직접 확인하기 위해 팀내에서 검수를 통한 라벨링을 진행하였고, 약 `5235개`의 에러 데이터가 있음을 확인하였습니다.
    - 다음 g2p가 진행된 라벨 데이터를 수정하기 위해 url을 통한 크롤링을 진행하였고 크롤링을 통해 모은 데이터를 에러 데이터와 검수하는 작업을 진행하였습니다.

<aside>
💡 1) **유기오전쟁 美 참저뇽사 추모벽 껄리반 상원 통과 RFA**

⇒ **6·25전쟁 美 참전용사 추모벽 건립안 상원 통과 RFA**

2) **국쩨 원자재 가격 꼉제에 미치느 녕향녁 깜소**

**⇒ 국제 원자재 가격 경제에 미치는 영향력 감소**

</aside>

![Untitled](Wrap-Up%20Report%203f64596467b54153913bf0533e554314/Untitled.png)

- **텍스트 길이**
    - 학습 데이터를 분석하면서 전체적으로 텍스트들의 길이 분포를 확인해본 결과 **가장 긴 텍스트의 길이**가 **46글자**, **짧은 텍스트의 길이**는 **4글자** **(평균:27자)**임을 확인했습니다.
    - 이는 직접적으로 데이터 샘플링에는 활용할 수 없지만 토크나이저를 설정하는데 도움을 줄것이라 생각합니다.

---

## 2. 맞춤법 검사를 사용한 데이터 클리닝

- 일부 데이터를 확인해봤을 때, 문법 또는 띄어쓰기 등 오류가 있는 데이터가 있는 것을 확인하였습니다. 이런 일부 에러 데이터가 학습을 진행하는데 노이즈(noise)로서 학습 결과에 영향을 줄 수 있다는 생각에 파이썬 라이브러리를 통한 맞춤법 검사 전처리를 진행하고자 합니다.

- [py-hanspell 라이브러리](https://github.com/ssut/py-hanspell)
    - `네이버 맞춤법 검사기`를 이용한 한글 맞춤법 검사 라이브러리 입니다.
    - 좀더 정교한 데이터로 문장을 수정하여 학습에 활용할 수 있습니다.
    
    <aside>
    🖐️ 1) **日 온라인으로 중국인 비자신청 받는다**  ⇒ **日 온라인으로 중국인 비자 신청받는다**
    2) **어버이날 맑다가 흐려져…남부지방 옅은 황사** ⇒ **어버이날 맑다가 흐려져…남부 지방 옅은 황사**
    
    </aside>
    

- 학습 데이터에 대해 hanspell 라이브러리를 통한 맞춤법 검사 전처리를 진행하였고, 이전 train 데이터로 학습하기 전보다 향상된 결과를 확인할 수 있었습니다.
    
    
    |  | train(base) | train(hanspell) |
    | --- | --- | --- |
    | eval/macro f1 | 0.8367 | 0.8408 |
    | eval/loss | 0.5106 | 0.4936 |

---

## 3. 크롤링을 사용한 데이터 클리닝

- `train`데이터셋에 URL정보가 같이 들어있었기 때문에 크롤링을 통한 데이터 클리닝이 가능하다고 생각했습니다.
- 크롤링을 통해서, 새로운 데이터셋(`scrapping_cleaning_train_ver4`)을 만들고, `train`과 비교한 결과
    - 6504개의 text오류를 확인했습니다.
    - target(라벨)의 변화를 확인했습니다. 특히, 사회(2) 라벨의 데이터 수가 많이 감소했습니다.
- **실험결과**
    - `train`과 데이터 클리닝(**`scrapping_cleaning_train_ver4`**)을 하기 전과 비교했을때, 제출스코어가 많이 하락했습니다.
    - 그 이유로는, TC작업이 언론사가 설정한 카테고리를 맞추는 것이 아니라, annotator들이 text를 보고 생각한 카테고리를 맞추는 작업이기 때문에 발생한 문제라고 생각했습니다.
    

| IT과학 (0) | 5309 -> 7852 | 경제 (1) | 6119 -> 6021 |
| --- | --- | --- | --- |
| 사회 (2) | 5180 -> 2099 | 생활문화 (3) | 5760 -> 6035 |
| 세계 (4) | 8250 -> 8030 | 스포츠 (5) | 7688 -> 7653 |
| 정치 (6) | 7988 -> 7372 |  |  |

| Dataset | f1 score | accuracy |
| --- | --- | --- |
| train | 0.8720 | 0.8698 |
| scrapping_cleaning_train_ver4 | 0.7817 (-0.0903) | 0.7656 (-1.042) |
| scrapping_cleaning_only_text | 0.8762(+0.0042) | 0.8761(+0.0063) |

---

## 4. ChatGPT 데이터 증강

- 클린랩을 통하여 모델의 예측확률을 기반으로 강하게 확신했지만 오답인 데이터들을 모아 모델이 헷갈리는 문장에 대한 고찰을 진행한 후 chatGPT 프롬프트에 이를 잘 구분하도록 유도하는 예시를 달라는 방식으로 증강을 시도하였습니다. 예를 들면 다음과 같습니다.
- 모델이 北과 南 같은 단어를 마주치면 바로 정치로 분류시킨다는 점을 여러 오답 데이터들에서 공통적으로 식별하였습니다.
- 따라서, chatGPT에 prompt로 이런 부분을 개선하도록 요구하여 증강데이터를 얻습니다.
- 이런 방식으로 취약점을 타켓팅한 160개 데이터를 우선 확보하여 테스트를 해본 결과,
    
    
    |          | F1 | Accuracy | rank |
    | --- | --- | --- | --- |
    | Base line | 0.8720 | 0.8698 |  |
    | Base line + Augmentation(160개) | 0.8792(+0.0072) | 0.8766(+0.0768) | SOTA(✔) |

|  text |  target |  pred |
| --- | --- | --- |
| 北여자 아이스하키 5명 출전 요구…南 3명으로 제한 |  스포츠 |  정치 |

![프롬프트.png](Wrap-Up%20Report%203f64596467b54153913bf0533e554314/%25ED%2594%2584%25EB%25A1%25AC%25ED%2594%2584%25ED%258A%25B8.png)

- 베이스라인 점수보다 많이 개선되었음을 확인할 수 있었습니다. confusion matrix 상으로 변화는 아래와 같습니다. 기존의 base line의 Confusion과 비교하여 표시한 박스들이며, 빨간색의 총 증가 개수보다 주황색의 총 감소 개수가 월등히 높아 성능도 많이 개선됨을 확인하였습니다.

![      **빨(증가) / 노(감소) / 초(유지)**](Wrap-Up%20Report%203f64596467b54153913bf0533e554314/%25EA%25B5%2590%25EC%25B0%25A8%25ED%2596%2589%25EB%25A0%25AC_%25EB%25B6%2584%25EC%2584%259D(1%25EC%25B0%25A8_%25EC%25A6%259D%25EA%25B0%2580_%25ED%259B%2584_valid_%25EA%25B3%25A0%25EC%25A0%2595)_%25EB%25B0%2595%25EC%258A%25A4%25ED%2591%259C%25EC%258B%259C_(1).png)

      **빨(증가) / 노(감소) / 초(유지)**

---

# 5. 최종 결과

- **평가 지표**는 **macro F1 score** 로 진행되었습니다.
    - **public** : 0.8821  ⇒ **private** : 0.8629
    
    ![Untitled](Wrap-Up%20Report%203f64596467b54153913bf0533e554314/Untitled%201.png)
    
    ![Untitled](Wrap-Up%20Report%203f64596467b54153913bf0533e554314/Untitled%202.png)
    
    | Model | optimizer | epoch | batch | learning rate |
    | --- | --- | --- | --- | --- |
    | monologg/kobert | AdamW | 2 | 32 | 2e-5 |
- public의 경우 가장 점수가 높았던 결과
    - **train 데이터를 hanspell 라이브러리로 전처리를 진행**한 결과가 SOTA
- private의 경우 가장 점수가 높았던 결과
    - **base train 데이터 + cleaning train 데이터를 hanspell 라이브러리 전처리 진행**한 결과가 SOTA

---

# 6. 자체  평가  의견

### A. **잘한 점들**

- 칸반보드, 노션과 같은 다양한 협업 툴을 활용한 것이 좋았습니다.
- 다양한 데이터 증강 기법들을 서칭하고 시도해보았습니다.
- 적절한 데이터 전처리 과정으로 학습 결과를 개선한 점에 만족스럽습니다.
- 직접 데이터 라벨링을 다시 진행한 클리닝 데이터를 활용한 것이 좋았습니다.
- 생성모델을 활용하여 데이터 증강을 시도한 경험을 하였습니다.

### B. **아쉬웠던  점들**

- 여러 이슈들로 인해 시간이 부족했습니다.
- API를 사용하는 task들은 물리적인 시간부족이 느껴졌습니다.
- 데이터 증강을 통한 추가적인 라벨링을 진행하지 못한 것이 아쉬웠습니다.
- label error cleaning에 대해 충분히 작업해 보지 못한 것이 아쉬웠습니다.
- 외부데이터를 사용하지 않은게 아쉬웠습니다.

---

### C. **시도  했으나  잘  되지  않았던  것들**

- Under-Sampling
- BackTranslation
- G2P Augmentation
- RD Augmentation

### D. **프로젝트를  통해  배운  점  또는  시사점**

- 모델만 집중하지 않고 데이터 중심으로 생각하는 법을 배웠습니다.
- 다양한 툴을 사용해 협업하는 경험을 할 수 있었습니다.
- 데이터 라벨링에 많은 시간이 요구됨을 확인하였고, 시간적 여유를 가질 필요성을 느꼈습니다.
